{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Embedding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM02YZTWIo7to7Pe+CgWiyw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Let us assume that we are given a pre-trained embedding matrix for an vocabulary of size 10. Each embedding vector in that matrix has dimension 4. Those dimensions are too small to be realistic and are only used for demonstration purposes:"],"metadata":{"id":"1ZfImCpUvktT"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PBRnMo_nvbhM","executionInfo":{"status":"ok","timestamp":1643827420104,"user_tz":300,"elapsed":132,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"eb4fe118-a7e9-4136-a61d-19fe2e7bb755"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.  1.  2.  3.]\n"," [ 4.  5.  6.  7.]\n"," [ 8.  9. 10. 11.]\n"," [12. 13. 14. 15.]\n"," [16. 17. 18. 19.]\n"," [20. 21. 22. 23.]\n"," [24. 25. 26. 27.]\n"," [28. 29. 30. 31.]\n"," [32. 33. 34. 35.]\n"," [36. 37. 38. 39.]]\n"]}],"source":["import numpy as np\n","\n","embedding_size = 4\n","vocab_size = 10\n","\n","embedding_matrix = np.arange(embedding_size * vocab_size, dtype='float32')\n","embedding_matrix = embedding_matrix.reshape(vocab_size, embedding_size)\n","print(embedding_matrix)"]},{"cell_type":"markdown","source":["To access the embedding for a given integer (ordinal) symbol , you may either:\n","\n","simply index (slice) the embedding matrix by i, using numpy integer indexing:"],"metadata":{"id":"5xtPYj0pvtIo"}},{"cell_type":"code","source":["i = 3\n","print(embedding_matrix[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3FRsRfevy7g","executionInfo":{"status":"ok","timestamp":1643827421729,"user_tz":300,"elapsed":141,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"664a100a-eb2a-4250-9c26-cf03e0f6e31e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[12. 13. 14. 15.]\n"]}]},{"cell_type":"markdown","source":["compute a one-hot encoding vector, then compute a dot product with the embedding matrix:"],"metadata":{"id":"jiD6OAQ9v1L1"}},{"cell_type":"code","source":["def onehot_encode(dim, label):\n","    return np.eye(dim)[label]\n","\n","\n","onehot_i = onehot_encode(vocab_size, i)\n","print(onehot_i)\n","embedding_vector = np.dot(onehot_i, embedding_matrix)\n","print(embedding_vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZL046l38v1t3","executionInfo":{"status":"ok","timestamp":1643827423182,"user_tz":300,"elapsed":141,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"99fef127-bd38-483f-cbfc-53a137d021d4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","[12. 13. 14. 15.]\n"]}]},{"cell_type":"markdown","source":["The Embedding layer in Keras\n","\n","In Keras, embeddings have an extra parameter, input_length which is typically used when having a sequence of symbols as input (think sequence of words). In our case, the length will always be 1.\n","\n","Embedding(output_dim=embedding_size, input_dim=vocab_size,\n","          input_length=sequence_length, name='my_embedding')\n","\n","furthermore, we load the fixed weights from the previous matrix instead of using a random initialization:\n","\n","Embedding(output_dim=embedding_size, input_dim=vocab_size,\n","          weights=[embedding_matrix],\n","          input_length=sequence_length, name='my_embedding')\n"],"metadata":{"id":"gCjA_i_7v8i9"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding\n","\n","embedding_layer = Embedding(\n","    output_dim=embedding_size, input_dim=vocab_size,\n","    weights=[embedding_matrix],\n","    input_length=1, name='my_embedding')"],"metadata":{"id":"f84Rl3sVv_FZ","executionInfo":{"status":"ok","timestamp":1643827427573,"user_tz":300,"elapsed":108,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Let's use it as part of a Keras model:"],"metadata":{"id":"b_uNRvkSwLOn"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","\n","x = Input(shape=[1], name='input')\n","embedding = embedding_layer(x)\n","model = Model(inputs=x, outputs=embedding)"],"metadata":{"id":"8yBnsot6wMDv","executionInfo":{"status":"ok","timestamp":1643827429052,"user_tz":300,"elapsed":395,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["The output of an embedding layer is then a 3-d tensor of shape (batch_size, sequence_length, embedding_size)."],"metadata":{"id":"kO3RCMfYwO-Y"}},{"cell_type":"code","source":["model.output_shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZI1aECSwP_-","executionInfo":{"status":"ok","timestamp":1643827430504,"user_tz":300,"elapsed":121,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"393902ff-3b89-4492-a1ee-56df7c127f5c"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(None, 1, 4)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["The embedding weights can be retrieved as model parameters:"],"metadata":{"id":"X4Ar8zIcwSTt"}},{"cell_type":"code","source":["model.get_weights()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qp_EBQEXwXq5","executionInfo":{"status":"ok","timestamp":1643827432193,"user_tz":300,"elapsed":143,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"91498d57-22d9-4162-bf91-a16eb85b1559"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.],\n","        [20., 21., 22., 23.],\n","        [24., 25., 26., 27.],\n","        [28., 29., 30., 31.],\n","        [32., 33., 34., 35.],\n","        [36., 37., 38., 39.]], dtype=float32)]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The model.summary() method gives the list of trainable parameters per layer in the model:"],"metadata":{"id":"uCg8DfNiwZyR"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qrlWvoKwb0X","executionInfo":{"status":"ok","timestamp":1643827433643,"user_tz":300,"elapsed":135,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"01bfdf01-ed01-4a61-a21d-9836be0f1a38"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input (InputLayer)          [(None, 1)]               0         \n","                                                                 \n"," my_embedding (Embedding)    (None, 1, 4)              40        \n","                                                                 \n","=================================================================\n","Total params: 40\n","Trainable params: 40\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["We can use the predict method of the Keras embedding model to project a single integer label into the matching embedding vector:"],"metadata":{"id":"BuS7zBEwweuE"}},{"cell_type":"code","source":["labels_to_encode = np.array([[3]])\n","model.predict(labels_to_encode)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iB9NCgIfwfRL","executionInfo":{"status":"ok","timestamp":1643827435599,"user_tz":300,"elapsed":471,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"916c27af-a8bb-4415-e2fb-978e50826139"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[12., 13., 14., 15.]]], dtype=float32)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Let's do the same for a batch of integers:"],"metadata":{"id":"iS6wF4bgwqWj"}},{"cell_type":"code","source":["labels_to_encode = np.array([[3], [3], [0], [9]])\n","model.predict(labels_to_encode)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16NGOlj_wq2n","executionInfo":{"status":"ok","timestamp":1643827437565,"user_tz":300,"elapsed":123,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"af7880db-f900-4c42-eb08-39218cd3172d"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[12., 13., 14., 15.]],\n","\n","       [[12., 13., 14., 15.]],\n","\n","       [[ 0.,  1.,  2.,  3.]],\n","\n","       [[36., 37., 38., 39.]]], dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["The output of an embedding layer is then a 3-d tensor of shape (batch_size, sequence_length, embedding_size). To remove the sequence dimension, useless in our case, we use the Flatten() layer"],"metadata":{"id":"U9uv7P6Lwvot"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Flatten\n","\n","x = Input(shape=[1], name='input')\n","y = Flatten()(embedding_layer(x))\n","model2 = Model(inputs=x, outputs=y)\n","model2.output_shape\n","model2.predict(np.array([3]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eoOnYi5NwvU_","executionInfo":{"status":"ok","timestamp":1643827457842,"user_tz":300,"elapsed":290,"user":{"displayName":"Jin Lu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08833536120631265770"}},"outputId":"94319f65-3463-4363-b9de-8e83f27c8746"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[12., 13., 14., 15.]], dtype=float32)"]},"metadata":{},"execution_count":14}]}]}